import os
import dspy
from dspy import InputField, OutputField, ChainOfThought, Predict, Signature, Module
from dotenv import load_dotenv
from load_data import get_query_engine, load_data

load_dotenv()

lm = dspy.GROQ(model='llama-3.1-8b-instant', api_key = os.getenv("GROQ_API_KEY") )


dspy.settings.configure(lm=lm)

index = load_data("data")
query_engine = get_query_engine(index)


# I want to create a project where a source document (PDF) is used to evaluate the responses generated by a large language model (LLM). The process works like this: the LLM's response will be broken down into paragraphs, and each paragraph will be searched for in the PDF using a Chroma/vector database retriever. A separate LLM agent will then assess whether the generated response is accurate based on the content of the PDF.

# So the data is loaded into a vector database. The user inputs a query (containing the response from LLM by his/her choice) and the Model will search the vector database for the relevant paragraphs. It will then tell if the response is accurate or not, if it is not accurate, it will give a detailed explanation as to why it is not accurate and then give a new response. This process will repeat until the response is accurate.

class compare_response(Signature):
    """Compare response accuracy to context. Answer accuracy score between (1-10) 10 being the highest accuracy"""
    initial_response = InputField(desc="The response to be assessed")
    context_response = InputField(desc="The response to be assessed")
    accuracy = OutputField(desc="accuracy score between (1-10)")
  
class response_generator(Signature):
    """ Generate a response to a given query given the context"""
    query = InputField(desc="Unfactual response")
    context = InputField(desc="May contain information about the response")
    response = OutputField(desc="New response for the query based on the context")

class Assessor(Module):
    def __init__(self):
        super().__init__()
        self.query_engine = query_engine
        self.generate_response = ChainOfThought(compare_response)
        
    def forward(self, initial_response: str) -> str:
        context = self.query_engine.query(initial_response).response
        return self.generate_response(
            initial_response=initial_response, 
            context_response=context
        )
    
class Generator(Module):
    def __init__(self):
        super().__init__()
        self.generate_response = ChainOfThought(response_generator)
        
    def forward(self, query: str, context: str) -> str:
        return self.generate_response(query=query, context=context)
    

if __name__ == "__main__":
    query = "One of the key professional failings during the 1996 disaster was the inability of the teams to adapt to changing conditions. Mount Everest is an inherently unpredictable environment, and successful climbers must be able to adjust their plans based on the realities they face on the mountain. However, both the New Zealand and American teams failed to do so."
    assessor  = Assessor()
    generator = Generator()
    
    response = assessor(initial_response=query)
    print(response)
    print(response.accuracy)
    
    
    
    
    
    
